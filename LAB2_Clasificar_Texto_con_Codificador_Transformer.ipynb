{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znvi_t1QUnAW"
      },
      "source": [
        "# Enunciado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_-9b5LEUsGS"
      },
      "source": [
        "**Utilizar un Codificador Transformer para clasificar las noticias de Reuters** en **46 temas mutuamente excluyentes**. Como cada noticia debe clasificarse en una sola categoría, es un problema de **\"clasificación multiclase de una sola etiqueta\"**.\n",
        "\n",
        "El Reuters dataset es un conjunto de noticias breves y sus temas, publicado por Reuters en 1986. Son 46 temas diferentes; **algunos temas están más representados que otros**, pero cada uno tiene, al menos, **10 ejemplos en el conjunto** de entrenamiento.\n",
        "\n",
        "Al igual que IMDB y MNIST, el conjunto de datos de Reuters viene **empaquetado como parte de Keras**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLSvfndsUvz6"
      },
      "source": [
        "# Desarrollo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxYaG2ESUypk"
      },
      "source": [
        "Crearemos lo que se llama **Transformer Encoder**, uno de los componentes básicos de la arquitectura de Transformer, y lo aplicaremos a la tarea de clasificación de noticias de Reuters.\n",
        "\n",
        "El siguiente código implementará un Transformer Encoder para clasificar noticias de Reuters en 46 categorías utilizando TensorFlow y Keras. Se prestará especial atención al entrenamiento del modelo y el ajuste de los hiperparámetros, según sea necesario.\n",
        "\n",
        "Primero, necesitamos importar los módulos necesarios y definir algunas constantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hMgpj6T8Tk5m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.datasets import reuters\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import TextVectorization, MultiHeadAttention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VewpzpSZVB3G"
      },
      "source": [
        "**Cargar y preparar los datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhawFd9lTs55",
        "outputId": "0bb18a3a-21a7-44e1-c70b-ffa66081c0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2110848/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Cargar el dataset Reuters\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "\n",
        "# Dividir el conjunto de entrenamiento en subconjuntos de entrenamiento y validación\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=1337)\n",
        "\n",
        "# Preparar los conjuntos de datos para su uso con Keras\n",
        "maxlen = 500\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen)\n",
        "val_data = keras.preprocessing.sequence.pad_sequences(val_data, maxlen=maxlen)\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "# Convertir las etiquetas a formato one-hot\n",
        "num_classes = np.max(train_labels) + 1\n",
        "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
        "val_labels = keras.utils.to_categorical(val_labels, num_classes)\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Crear datasets con los conjuntos de entrenamiento, validación y prueba\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels)).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UBFynT9VWT9"
      },
      "source": [
        "**Vectorizar los datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "prGEEPFETxYl"
      },
      "outputs": [],
      "source": [
        "# Define the text vectorization parameters\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "# Obtener los datos y las etiquetas del conjunto de entrenamiento\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for x, y in train_dataset:\n",
        "    train_texts.extend([\" \".join([str(word_index) for word_index in sample]) for sample in x.numpy()])\n",
        "    train_labels.extend(y.numpy())\n",
        "\n",
        "# Adapt the TextVectorization layer to the training texts\n",
        "text_vectorization.adapt(train_texts)\n",
        "\n",
        "# Crear dataset de entrenamiento con texto vectorizado\n",
        "int_train_ds = text_vectorization(np.array(train_texts))\n",
        "\n",
        "# Crear dataset de entrenamiento final\n",
        "int_train_ds = tf.data.Dataset.from_tensor_slices((int_train_ds, train_labels)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-HsQctBWR9A"
      },
      "source": [
        "### El Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ámbito del procesamiento de lenguaje natural, especialmente para tareas como la clasificación de textos —por ejemplo, determinar si una reseña de película es positiva o negativa utilizando datasets como reuters.load_data() en Keras—, tres componentes juegan roles fundamentales:\n",
        "\n",
        "1. *Positional Embedding*: Esta técnica aporta información sobre el orden de las palabras dentro de una secuencia de texto a los modelos de transformadores, que por su diseño no capturan secuencialidad de forma inherente. Al sumar la información posicional al embedding de cada palabra, el modelo puede entender la estructura del texto y la relevancia del orden de las palabras, lo cual es crucial para interpretar correctamente el significado de las oraciones y párrafos.\n",
        "\n",
        "2. *Transformer Encoder*: Constituye el bloque constructivo central de un modelo de transformador, procesando secuencias de entrada para crear representaciones contextualizadas del texto. Compuesto por múltiples capas que incluyen atención auto-dirigida y redes neuronales feed-forward, el encoder permite al modelo ponderar la importancia relativa de diferentes palabras basándose en su contexto. Esto facilita una comprensión profunda del texto, crucial para analizar y clasificar reseñas de películas donde el contexto de cómo se usan las palabras puede cambiar completamente su significado.\n",
        "\n",
        "3. *Embedding*: Los embeddings transforman palabras en vectores de números, proporcionando una representación densa y significativa de cada palabra basada en su uso y contexto lingüístico. Estos vectores son esenciales para que los modelos de aprendizaje automático procesen texto, ya que les permiten operar con datos numéricos y aprender patrones asociados con diferentes clasificaciones de texto. Los embeddings pueden ser específicamente ajustados para una tarea, mejorando la capacidad del modelo para entender y clasificar correctamente las reseñas de películas según su contenido.\n",
        "\n",
        "En conjunto, estos componentes habilitan a los modelos de transformadores para analizar textos complejos, como reseñas de películas, capturando tanto la estructura lingüística como el significado contextual de las palabras, lo cual es vital para realizar clasificaciones precisas y significativas."
      ],
      "metadata": {
        "id": "0Bl2TF0nj1vP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiXXEQVsVfsM"
      },
      "source": [
        "**Transformer Encoder implementado como una subclase de `Layer`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "khnyUNP7ZFL7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Tamaño de los vectores de los tokens de entrada\n",
        "        self.embed_dim = embed_dim\n",
        "        # Tamaño de la capa densa interna\n",
        "        self.dense_dim = dense_dim\n",
        "        # Número de attention heads\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"sigmoid\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # El cálculo va en call()\n",
        "    def call(self, inputs, mask=None):\n",
        "        # La máscara que generará la capa Embedding\n",
        "        # será 2D, pero la capa de atención espera\n",
        "        # ser 3D o 4D, por lo que ampliamos su rango\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    # Implementamos la serialización para\n",
        "    # que podamos guardar el modelo\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QOa59_1ZXWE"
      },
      "source": [
        "VARIACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g-plf6l8ZM8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "d261e843-c49e-4c43-9d5c-961ab016eb99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 46)                11822     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3115598 (11.89 MB)\n",
            "Trainable params: 3115598 (11.89 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            " 28/225 [==>...........................] - ETA: 6:23 - loss: 2.8268 - accuracy: 0.3616"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-21d860983771>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transformer_encoder.keras\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"TransformerEncoder\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "vocab_size = 10000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "num_classes = 46  # Asumiendo que hay 46 clases únicas en el conjunto de datos de Reuters\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Configuración de callbacks para la regularización y optimización del entrenamiento\n",
        "model_callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder_best.keras\", save_best_only=True),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),  # Aumento de la paciencia\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)  # Ajuste dinámico de la tasa de aprendizaje\n",
        "]\n",
        "\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=model_callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "# Evaluar el modelo en el conjunto de datos de prueba\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "print(f\"Loss en el conjunto de prueba: {loss}\")\n",
        "print(f\"Precisión en el conjunto de prueba: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CElUs6eVT0hm"
      },
      "source": [
        "Ahora, implementaremos el Transformer Encoder y la incrustación posicional como subclases de Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UgjZgZULT16g"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj-Uz0jGV3ph"
      },
      "source": [
        "**Usando el Transformer encoder para clasificación de texto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v99CyVcfT5kg"
      },
      "source": [
        "Finalmente, para crear y entrenar un modelo combinando el Transformer Encoder con incrustación posicional (positional embedding) con el fin de conseguir un modelo de clasifiación de texto, primero necesitamos definir el modelo en Keras. Luego, podemos compilarlo y entrenarlo utilizando los datos preparados. Aquí está el proceso paso a paso:\n",
        "\n",
        "Definir el modelo:\n",
        "\n",
        "Añadir una capa de entrada para los datos de entrada.\n",
        "Añadir una capa de incrustación posicional para inyectar información de posición.\n",
        "Añadir un TransformerEncoder para procesar las secuencias de entrada.\n",
        "Añadir una capa de reducción de dimensionalidad como GlobalMaxPooling1D o similar.\n",
        "Añadir una capa Dropout para regularización.\n",
        "Añadir una capa de salida con activación softmax para la clasificación multiclase.\n",
        "Compilar el modelo:\n",
        "\n",
        "Especificar la función de pérdida (por ejemplo, categorical_crossentropy para la clasificación multiclase).\n",
        "Seleccionar un optimizador (por ejemplo, rmsprop).\n",
        "Especificar las métricas a seguir durante el entrenamiento (por ejemplo, accuracy).\n",
        "Entrenar el modelo:\n",
        "\n",
        "Utilizar los conjuntos de datos preparados (train_dataset, val_dataset, test_dataset) para entrenamiento, validación y evaluación respectivamente.\n",
        "Especificar el número de épocas y cualquier otro hiperparámetro relevante.\n",
        "Aquí está el código para llevar a cabo estos pasos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxH03YIvT6w_"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)  # Cambiado a num_classes\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",  # Cambiado a categorical_crossentropy\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USMAG9LBWCtx"
      },
      "source": [
        "**Entrenamiento y evaluación del modelo basado en Transformer Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq4hIZJIXzpi"
      },
      "outputs": [],
      "source": [
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de continuar el documento y pasar a probar nuevas modificaciones de los modelos, extraeremos ciertas conclusiones de estos dos primeros.\n",
        "\n",
        "El primer modelo, en el que se trabaja con *Embedding*, se ronda el 81% de precisión. Por tanto, debido a que el código base estudiado en clase ronda el 70%, es posible afirmar que la elección en la variación de parámetros ha conllevado un aumento en la nueva precisión."
      ],
      "metadata": {
        "id": "RpoB9JCMlCuS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzdGR_z8ZfkG"
      },
      "source": [
        "## Ejemplos alternativos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHYo9WciZoRZ"
      },
      "source": [
        "### Variación de hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWCHowx2bHg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b8e57f-a973-4c6f-8a8c-c0f49a82b67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posi  (None, None, 256)         2713600   \n",
            " tionalEmbedding)                                                \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tra  (None, None, 256)         1069600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 46)                11822     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3795022 (14.48 MB)\n",
            "Trainable params: 3795022 (14.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "225/225 [==============================] - 947s 4s/step - loss: 2.1746 - accuracy: 0.5307 - val_loss: 1.3439 - val_accuracy: 0.7028\n",
            "Epoch 2/20\n",
            "225/225 [==============================] - 961s 4s/step - loss: 1.2944 - accuracy: 0.7084 - val_loss: 1.0905 - val_accuracy: 0.7513\n",
            "Epoch 3/20\n",
            "225/225 [==============================] - 1026s 5s/step - loss: 0.9672 - accuracy: 0.7752 - val_loss: 0.9868 - val_accuracy: 0.7830\n",
            "Epoch 4/20\n",
            "225/225 [==============================] - 954s 4s/step - loss: 0.7667 - accuracy: 0.8182 - val_loss: 0.9577 - val_accuracy: 0.7919\n",
            "Epoch 5/20\n",
            "212/225 [===========================>..] - ETA: 51s - loss: 0.6345 - accuracy: 0.8474"
          ]
        }
      ],
      "source": [
        "vocab_size = 10000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 4 # Ejemplo 1: Cambiar el número de cabezas de atención en el TransformerEncoder\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlslYemPbiPc"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "sequence_length = 600\n",
        "embed_dim = 512 # Ejemplo 2: Modificar la dimensionalidad de las incrustaciones\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzIUInHyZsa0"
      },
      "source": [
        "### Arquitecturas alternativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-1WHrs5b0rJ"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "# Ejemplo 1: Añadir capas convolucionales después del TransformerEncoder\n",
        "x = layers.Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usr2t0Asb9j3"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "# Ejemplo 2: Utilizar una red neuronal recurrente (LSTM) en lugar del TransformerEncoder\n",
        "x = layers.LSTM(128)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHgQRUm1cHvK"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "# Ejemplo 3: Combinar una CNN con una capa LSTM\n",
        "cnn_output = layers.Conv1D(filters=64, kernel_size=3, activation='relu')(x)\n",
        "lstm_output = layers.LSTM(64)(cnn_output)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SySiwMaZvrf"
      },
      "source": [
        "### Técnicas de regularización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaJxyNmhcSTZ"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Ejemplo 1: Añadir regularización L2 a las capas densas\n",
        "from keras import regularizers\n",
        "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zFUVOceZ0vA"
      },
      "source": [
        "### Cambiar el optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI6SaSNhdLX2"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\", # Ejemplo 1\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra01nkcCZ3b6"
      },
      "source": [
        "### Ajustar la tasa de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJtu8SMXdjcZ"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIi2bqMsZ591"
      },
      "source": [
        "### Añadir programación de tasa de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAuk5B_Kd9aR"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co081kqFZ9Qq"
      },
      "source": [
        "### Aumentar el número de épocas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vnizIAUeBj4"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=30, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BixZVIZZ_zZ"
      },
      "source": [
        "### Utilizar callbacks adicionales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "885OEahZeU5h"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_with_positional_embedding.keras\",\n",
        "                                    save_best_only=True),\n",
        "    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2)\n",
        "]\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=callbacks)\n",
        "\n",
        "# Evaluar el modelo\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_with_positional_embedding.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test accuracy: {model.evaluate(test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos concluido que  la proliferación de modelos nos ayuda a tener una visión amplia de los reusltado. Sin embargo, debido a la gran carga computacional y al entorno no es capaz de soportarlo por ello hemos decidido no realizarlo en este ejercicio. En un entorno mejor donde se pudiera ver con claridad, se podría observar las diferencias entre modelos por ello hemos querido dejar todos los modelos creados para que se pueda ver el trabajo detrás de ellos, aunque no se hayan podido compilar\n",
        "\n",
        "No obstante, se pueden sacar conclusiones determinantes con el fin de encontrar el mejor modelo a partir de la estratégica modificación de las especificaciones de los modelos y el cambio en la precisión final que estos provocan"
      ],
      "metadata": {
        "id": "q-MarqSLjnCr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}